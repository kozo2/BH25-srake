@article{Choudhary2019,
doi = {10.12688/f1000research.18676.1},
url = {https://doi.org/10.12688/f1000research.18676.1},
year = {2019},
month = apr,
publisher = {F1000 (Faculty of 1000 Ltd)},
volume = {8},
pages = {532},
author = {Saket Choudhary},
title = {pysradb: A {P}ython package to query next-generation sequencing metadata and data from {NCBI} {S}equence {R}ead {A}rchive},
journal = {F1000Research}
}

@article{zhuSRAdbQueryUse2013,
  title = {{{SRAdb}}: Query and Use Public next-Generation Sequencing Data from within {{R}}},
  shorttitle = {{{SRAdb}}},
  author = {Zhu, Yuelin and Stephens, Robert M. and Meltzer, Paul S. and Davis, Sean R.},
  year = {2013},
  month = jan,
  journal = {BMC Bioinformatics},
  volume = {14},
  number = {1},
  pages = {19},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-14-19},
  url = {https://doi.org/10.1186/1471-2105-14-19},
  abstract = {The Sequence Read Archive (SRA) is the largest public repository of sequencing data from the next generation of sequencing platforms including Illumina (Genome Analyzer, HiSeq, MiSeq, .etc), Roche 454 GS System, Applied Biosystems SOLiD System, Helicos Heliscope, PacBio RS, and others.},
  keywords = {Fastq File,Full Text Search,Genomic Viewer,Library Strategy,Sequence Read Archive},
  file = {/Users/nishad/Zotero/storage/IHH9NS57/Zhu et al. - 2013 - SRAdb query and use public next-generation sequen.pdf}
}

@article{ffq,
  title={Metadata retrieval from sequence databases with ffq},
  author={G{\'a}lvez-Merch{\'a}n, {\'A}ngel and Min, Kyung Hoi Joseph and Pachter, Lior and Booeshaghi, A. Sina},
  year={2022}
}


@misc{thalhath_2024,
 title={Revisiting SRAmetadb.sqlite},
 url={osf.io/preprints/biohackrxiv/xvrcm_v1},
 DOI={10.37044/osf.io/xvrcm},
 publisher={BioHackrXiv},
 author={Thalhath, Nishad},
 year={2024},
 month={Aug}
}


@inproceedings{sapbert,
    title = "Self-Alignment Pretraining for Biomedical Entity Representations",
    author = "Liu, Fangyu  and
      Shareghi, Ehsan  and
      Meng, Zaiqiao  and
      Basaldella, Marco  and
      Collier, Nigel",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.334/",
    doi = "10.18653/v1/2021.naacl-main.334",
    pages = "4228--4238",
    abstract = "Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust."
}
